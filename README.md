# Papers
## Attack
### Poisoning
1. Transferable Clean-Label Poisoning Attacks on Deep Neural Nets
[~pdf](https://arxiv.org/pdf/1905.05897.pdf)
### Backdoor attack
1. Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning[~pdf](https://arxiv.org/pdf/1712.05526.pdf?source=post_page---------------------------)
### Attack in other domains
1. DBA: Distributed Backdoor Attacks Against Federated Learning[~pdf](https://openreview.net/attachment?id=rkgyS0VFvr&name=original_pdf)


## Defense

### Adversarial learning variants
1. Theoretically Principled Trade-off between Robustness and Accuracy[~pdf](https://arxiv.org/pdf/1901.08573.pdf)
2. You only propagate once: Accelerating adversarial training via maximal principle[~pdf](http://papers.nips.cc/paper/8316-you-only-propagate-once-accelerating-adversarial-training-via-maximal-principle)
3. Fast is better than free: Revisiting adversarial training[~pdf](https://arxiv.org/pdf/2001.03994.pdf)
### Provalable defense
1. Certified Adversarial Robustness via Randomized Smoothing[~pdf](https://arxiv.org/pdf/1902.02918.pdf)
## Others
1. Adverserial Examples are Not Bugs, They are Features.
[~pdf](https://arxiv.org/abs/1905.02175)  [~Intro](https://github.com/I-am-Bot/Papers/blob/master/1.md)
2. Rethinking the security of skip connections in resent-like neural networks.
[~pdf](https://openreview.net/pdf?id=BJlRs34Fvr)  [~Intro](https://github.com/I-am-Bot/Papers/blob/master/Rethinking%20the%20security%20of%20skip%20connections%20in%20resent-like%20neural%20networks.md)

